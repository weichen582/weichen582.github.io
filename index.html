<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Chen Wei</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images_new/JHU_icon.jpg">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chen Wei</name>
              </p>
              <p>
                I am a final-year Computer Science Ph.D. student at Johns Hopkins University,
                where I am fortunate to be advised by  <a href="https://en.wikipedia.org/wiki/Bloomberg_Distinguished_Professorships">Bloomberg Distinguished Professor</a>
               <a href="https://cs.jhu.edu/~ayuille/">Alan L. Yuille</a>. I received my B.S. with honors from <a href="https://english.pku.edu.cn/">Peking University</a>.
               I spent great time at <a href="https://deepmind.google/">Google DeepMind</a>, <a href="https://ai.meta.com/research/">FAIR @ Meta</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:weichen3012@gmail.com">weichen3012 [at] gmail [dot] com</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=LHQGpBUAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/ChenWei23787135">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%;">
              <a href="images_new/profile_2.jpg"><img style="width:100%;max-width:100%;border-radius:50%;" alt="profile photo" src="images_new/profile_2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-top:20px;padding-bottom:20px;width:100%;vertical-align:middle">
              <heading>Research Interest</heading>
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              My research lies at the intersection of <b>machine learning</b> and <b>computer vision</b>, driven by the goal of developing flexible and general-purpose intelligent systems.
              My research has primarily focused on two critical steps toward this goal, (1) to encode enormously complex raw visual signals to structured representations that are easier to understand and process for machine, and (2) to connect visual representations to higher-order cognition and commonsense reasoning about the world.
            </td>
          </tr>

          <tr>
            <td width="100%" valign="middle">
              <ul>
                <li><b>Unsupervised Learning</b>: discovering natural structures in the raw data to obtain structured representation that is scalable to wide domains and applicable to a variety tasks.</li>
              </ul>

              <ul>
                <li><b>Generative Understanding</b>: constructing and learning from generative models of the world.</li>
              </ul>

              <ul>
                <li><b>Vision and Language</b>: connecting visual representations to commonsense knowledge base like large language models to achieve higher-order cognition.</li>
              </ul>
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              <span style="color:red">I am on the job market for 2024!</span> Would love to chat more if you are interested. I am also happy to give talks on my research in related seminars.
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-top:20px;padding-bottom:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              [11/01/2023] Check out the <a href="https://dediffusion.github.io/">cool examples</a> from <a href="https://arxiv.org/abs/2311.00618">De-Diffusion Makes Text a Strong Cross-Modal Interface</a>!
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              [10/31/2023] Gave a talk at <a href="https://sites.google.com/view/visionseminar">MIT Visual Computing Seminar</a>.
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              [10/27/2023] Gave a talk at <a href="https://cvl.psych.ucla.edu/">The Computational Vision and Learning Lab @ UCLA</a>.
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              [08/29/2023] I have been selected as one of the <a href="https://eecsrisingstars2023.cc.gatech.edu/">EECS Rising Stars 2023</a>.
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              [07/16/2023] <a href="https://weichen582.github.io/diffmae.html">DiffMAE</a> and <a href="https://arxiv.org/abs/2211.11446">SMAUG</a> accepted to ICCV 2023.
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              [04/23/2023] <a href="https://github.com/facebookresearch/hiera">Hiera</a> accepted to ICML 2023 as oral presentation.
            </td>
          </tr>


          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              [04/06/2023] Excited to release our new work on reformulating <a href="https://arxiv.org/abs/2304.03283">Diffusion Models as Masked Autoencoders</a>.
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              [10/19/2022] Proud to be an <a href="https://eccv2022.ecva.net/program/outstanding-reviewers/">ECCV 2022 outstanding reviewer</a>.
            </td>
          </tr>

          <tr>
            <td style="padding-bottom:10px;width:100%;vertical-align:middle">
              [09/07/2022] Passed my Graduate Board Oral exam and now a PhD candidate :) 
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-top:20px;padding-bottom:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          <tr onmouseout="dediffusion_stop()" onmouseover="dediffusion_start()" bgcolor="#ffffd0">
            <td style="padding:40px;padding-top:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dediffusion_image'><img style="width: 120%; height: auto;" src='images_new/dediffusion_text.png'></div>
                <img style="width: 120%; height: auto;" src='images_new/corgi.jpeg'>
              </div>
              <script type="text/javascript">
                function dediffusion_start() {
                  document.getElementById('dediffusion_image').style.opacity = "1";
                }

                function dediffusion_stop() {
                  document.getElementById('dediffusion_image').style.opacity = "0";
                }
                dediffusion_stop()
              </script>
            </td>

            <td style="padding:40px;padding-top:70px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/abs/2311.00618">
                <papertitle> De-Diffusion Makes Text a Strong Cross-Modal Interface </papertitle>
              </a>
              <br>
              <b>Chen Wei</b>,
              <a href="https://www.cs.jhu.edu/~cxliu/", class="author">Chenxi Liu</a>,
              <a href="https://www.cs.jhu.edu/~syqiao/", class="author">Siyuan Qiao</a>,
              <a href="https://zhishuai.xyz/", class="author">Zhishuai Zhang</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
              <a href="https://jiahuiyu.com/", class="author", class="author">Jiahui Yu</a>
              <br>
              <em>arXiv </em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2311.00618">arXiv</a>  /
              <a href="https://dediffusion.github.io/">project page</a>
              <br>
            </td>
          </tr>

          <tr onmouseout="diffmae_stop()" onmouseover="diffmae_start()" bgcolor="#ffffd0">
            <td style="padding:40px;padding-top:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='diffmae_image'><img style="width: 120%; height: auto;" src='diffmae/interpolation/input.png'></div>
                <img style="width: 120%; height: auto;" src='diffmae/interpolation/2.png'>
              </div>
              <script type="text/javascript">
                function diffmae_start() {
                  document.getElementById('diffmae_image').style.opacity = "1";
                }

                function diffmae_stop() {
                  document.getElementById('diffmae_image').style.opacity = "0";
                }
                diffmae_stop()
              </script>
            </td>

            <td style="padding:40px;padding-top:70px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/abs/2304.03283">
                <papertitle> Diffusion Models as Masked Autoencoders </papertitle>
              </a>
              <br>
              <b>Chen Wei</b>,
              <a href="https://karttikeya.github.io/", class="author">Karttikeya Mangalam</a>,
              <a href="http://www.cs.cmu.edu/~poyaoh/", class="author">Po-Yao Huang</a>,
              <a href="https://lyttonhao.github.io/", class="author">Yanghao Li</a>,
              <a href="https://haoqifan.github.io/", class="author">Haoqi Fan</a>,
              <a href="https://howardhsu.github.io/", class="author">Hu Xu</a>,
              <a href="https://csrhddlam.github.io/", class="author">Huiyu Wang</a>,
              <a href="https://cihangxie.github.io/", class="author">Cihang Xie</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
              <a href="https://feichtenhofer.github.io/", class="author", class="author">Christoph Feichtenhofer</a>
              <br>
              <em>ICCV </em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2304.03283">arXiv</a>  /
              <a href="https://weichen582.github.io/diffmae.html">project page</a> /
              <a href="https://www.marktechpost.com/2023/04/11/a-new-ai-research-integrates-masking-into-diffusion-models-to-develop-diffusion-masked-autoencoders-diffmae-a-self-supervised-framework-designed-for-recognizing-and-generating-images-and-videos/">Marktechpost</a>
              <br>
            </td>
          </tr>

          <tr>
            <td style="padding:40px;padding-left:0px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='smaug_image'><img style="width: 150%; height: auto;" src='images_new/smaug.png'></div>
              </div>
            </td>

            <td style="padding:40px;padding-top:30px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.11446">
                <papertitle> SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-Training </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=0WFC2w0AAAAJ", class="author">Yuanze Lin</a>,
              <strong>Chen Wei</strong>,
              <a href="https://csrhddlam.github.io/", class="author">Huiyu Wang</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
              <a href="https://cihangxie.github.io/", class="author">Cihang Xie</a>
              <br>
              <em>ICCV </em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2211.11446">arXiv</a>
              <br>
            </td>
          </tr>

          <tr onmouseout="hiera_stop()" onmouseover="hiera_start()" bgcolor="#ffffd0">
            <td style="padding:40px;padding-left:30px;padding-top:0px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hiera_image'><img style="width: 110%; height: auto;" src='images_new/hiera_before.png'></div>
                <img style="width: 110%; height: auto;" src='images_new/hiera_after.png'>
              </div>
              <script type="text/javascript">
                function hiera_start() {
                  document.getElementById('hiera_image').style.opacity = "1";
                }

                function hiera_stop() {
                  document.getElementById('hiera_image').style.opacity = "0";
                }
                hiera_stop()
              </script>
            </td>

            <td style="padding:40px;padding-top:50px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.11709">
                <papertitle> Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=4LWx24UAAAAJ", class="author">Chaitanya Ryali</a>*,
              <a href="https://scholar.google.com/citations?user=aMpbemkAAAAJ", class="author">Yuan-Ting Hu</a>*,
              <a href="https://dbolya.github.io/", class="author">Daniel Bolya</a>*,
              <strong>Chen Wei</strong>,
              <a href="https://haoqifan.github.io/", class="author">Haoqi Fan</a>,
              <a href="http://www.cs.cmu.edu/~poyaoh/", class="author">Po-Yao Huang</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=Qwm6ZOYAAAAJ", class="author">Vaibhav Aggarwal</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=42v1i_YAAAAJ", class="author">Arkabandhu Chowdhury</a>,
              <a href="https://omidpoursaeed.github.io/", class="author">Omid Poursaeed</a>,
              <a href="https://faculty.cc.gatech.edu/~judy/", class="author">Judy Hoffman</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/", class="author">Jitendra Malik</a>,
              <a href="https://lyttonhao.github.io/", class="author">Yanghao Li</a>*,
              <a href="https://feichtenhofer.github.io/", class="author">Christoph Feichtenhofer</a>*
              <br>
              <em>ICML </em>, 2023 <strong>Oral</strong>
              <br>
              <a href="https://arxiv.org/abs/2306.00989/">arXiv</a>  /
              <a href="https://github.com/facebookresearch/hiera">code</a>
              <br>
            </td>
          </tr>

          <tr>
            <td style="padding:40px;padding-left:20px;padding-bottom:0px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dmae_image'><img style="width: 130%; height: auto;" src='images_new/dmae.png'></div>
              </div>
            </td>

            <td style="padding:40px;padding-top:10px;padding-bottom:0px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2208.12256">
                <papertitle> Masked Autoencoders Enable Efficient Knowledge Distillers </papertitle>
              </a>
              <br>
              <a class="author">Yutong Bai</a>,
              <a href="https://zeyuwang.netlify.app/", class="author">Zeyu Wang</a>,
              <a href="https://lambert-x.github.io/", class="author">Junfei Xiao</a>,
              <strong>Chen Wei</strong>,
              <a href="https://csrhddlam.github.io/", class="author">Huiyu Wang</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
              <a href="https://yuyinzhou.github.io/", class="author">Yuyin Zhou</a>,
              <a href="https://cihangxie.github.io/", class="author">Cihang Xie</a>
              <br>
              <em>CVPR </em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2208.12256">arXiv</a>  /
              <a href="https://github.com/UCSC-VLAA/DMAE">code</a>
              <br>
            </td>
          </tr>

          <tr onmouseout="cp2_stop()" onmouseover="cp2_start()">
            <td style="padding:40px;padding-left:30px;padding-top:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cp2_image'><img style="width: 120%; height: auto;" src='images_new/cp2_after.png''></div>
                <img style="width: 120%; height: auto;" src='images_new/cp2_before.png'>
              </div>
              <script type="text/javascript">
                function cp2_start() {
                  document.getElementById('cp2_image').style.opacity = "1";
                }

                function cp2_stop() {
                  document.getElementById('cp2_image').style.opacity = "0";
                }
                cp2_stop()
              </script>
            </td>

            <td style="padding:40px;padding-top:40px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.11709">
                <papertitle> CP2: Copy-Paste Contrastive Pretraining for Semantic Segmentation </papertitle>
              </a>
              <br>
              <a href="https://wangf3014.github.io/home/", class="author">Feng Wang</a>,
              <a href="https://csrhddlam.github.io/", class="author">Huiyu Wang</a>,
              <strong>Chen Wei</strong>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
              <a href="https://shenwei1231.github.io/", class="author">Wei Shen</a>
              <br>
              <em>ECCV </em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.11709", class="author">arXiv</a>  /
              <a href="https://github.com/wangf3014/CP2", class="author">code</a>
              <br>
            </td>
          </tr>

          <tr>
            <td style="padding:40px;padding-left:40px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='li_image'><img style="width: 120%; height: auto;" src='images_new/eccv2023li.png'></div>
              </div>
            </td>

            <td style="padding:40px;padding-top:50px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2205.01721">
                <papertitle> In Defense of Image Pre-Training for Spatiotemporal Recognition</papertitle>
              </a>
              <br>
              <a href="https://xhl-video.github.io/xianhangli/", class="author">Xianhang Li</a>,
              <a href="https://csrhddlam.github.io/", class="author">Huiyu Wang</a>,
              <strong>Chen Wei</strong>,
              <a href="https://meijieru.com/", class="author">Jieru Mei</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
              <a href="https://yuyinzhou.github.io/", class="author">Yuyin Zhou</a>,
              <a href="https://cihangxie.github.io/", class="author">Cihang Xie</a>
              <br>
              <em>ECCV </em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2205.01721">arXiv</a> /
              <a href="https://github.com/UCSC-VLAA/Image-Pretraining-for-Video">code</a>
              <br>
            </td>
          </tr>

          <tr onmouseout="maskfeat_stop()" onmouseover="maskfeat_start()" bgcolor="#ffffd0">
            <td style="padding:40px;padding-top:0px;padding-left:30px;width:30%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='maskfeat_image'><img style="width: 120%; height: auto;" src='images_new/maskfeat_after.png'></div>
                <img style="width: 120%; height: auto;" src='images_new/maskfeat_before.png'>
              </div>
              <script type="text/javascript">
                function maskfeat_start() {
                  document.getElementById('maskfeat_image').style.opacity = "1";
                }

                function maskfeat_stop() {
                  document.getElementById('maskfeat_image').style.opacity = "0";
                }
                maskfeat_stop()
              </script>
            </td>
            <td style="padding:40px;padding-top:40px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2112.09133.pdf">
                <papertitle> Masked Feature Prediction for Self-Supervised Visual Pre-Training </papertitle>
              </a>
              <br>
              <strong>Chen Wei*</strong>,
              <a href="https://haoqifan.github.io/", class="author">Haoqi Fan</a>,
              <a href="https://vcl.ucsd.edu/~sxie/", class="author">Saining Xie</a>,
              <a href="https://chaoyuan.org/", class="author">Chao-Yuan Wu</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
              <a href="https://feichtenhofer.github.io/", class="author">Christoph Feichtenhofer*</a>
              <br>
              <em>CVPR </em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2112.09133.pdf">arXiv</a>  /
              <a href="https://github.com/facebookresearch/SlowFast">code at pySlowFast</a>
              <br>
              <br>
              <a href="https://www.paperdigest.org/2023/04/most-influential-cvpr-papers-2023-04/">Most Influential CVPR Papers (2023-04)</a>
            </td>
          </tr>

          <tr onmouseout="ibot_stop()" onmouseover="ibot_start()" bgcolor="#ffffd0">
            <td style="padding:40px;padding-left:30px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ibot_image'><img style="width: 120%; height: auto;" src='images_new/ibot_after.png'></div>
                <img style="width: 120%; height: auto;" src='images_new/ibot_before.png'>
              </div>
              <script type="text/javascript">
                function ibot_start() {
                  document.getElementById('ibot_image').style.opacity = "1";
                }

                function ibot_stop() {
                  document.getElementById('ibot_image').style.opacity = "0";
                }
                ibot_stop()
              </script>
            </td>
            <td style="padding:40px;padding-top:80px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2111.07832.pdf">
                <papertitle> iBOT: Image BERT Pre-Training with Online Tokenizer </papertitle>
              </a>
              <br>
              <a href="https://shallowtoil.github.io/", class="author">Jinghao Zhou</a>,
              <strong>Chen Wei</strong>,
              <a href="https://csrhddlam.github.io/", class="author">Huiyu Wang</a>,
              <a href="https://shenwei1231.github.io/", class="author">Wei Shen</a>,
              <a href="https://cihangxie.github.io/", class="author">Cihang Xie</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
              <a href="http://www.taokong.org/", class="author">Tao Kong</a>
              <br>
              <em>ICLR </em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2111.07832.pdf">arXiv</a>  /
              <a href="https://github.com/bytedance/ibot">code</a> /
              <a href="https://medium.com/syncedreview/meet-ibot-a-masked-image-modelling-framework-that-enables-bert-like-pretraining-for-vision-da01002115e7">press</a>
              <br>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="crest_stop()" onmouseover="crest_start()" bgcolor="#ffffd0">
            <td style="padding:40px;padding-left:30px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='crest_image'><img style="width: 120%; height: auto;" src='images_new/crest_after.png'></div>
                <img style="width: 120%; height: auto;" src='images_new/crest_before.png'>
              </div>
              <script type="text/javascript">
                function crest_start() {
                  document.getElementById('crest_image').style.opacity = "1";
                }

                function crest_stop() {
                  document.getElementById('crest_image').style.opacity = "0";
                }
                crest_stop()
              </script>
            </td>
            <td style="padding:40px;padding-top:60px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2102.09559.pdf">
                <papertitle> CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning </papertitle>
              </a>
              <br>
              <strong>Chen Wei</strong>,
              <a href="https://sites.google.com/site/kihyuksml/", class="author">Kihyuk Sohn</a>,
              <a href="https://scholar.google.com/citations?user=Vu3vH2sAAAAJ&hl=en", class="author">Clayton Mellina</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
              <a href="http://users.umiacs.umd.edu/~fyang/", class="author">Fan Yang</a>
              <br>
              <em>CVPR </em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2102.09559.pdf">arXiv</a>  /
              <a href="https://github.com/google-research/crest">code</a> /
              <a href="files/CReST_poster.pdf">poster</a> /
              <a href="https://www.youtube.com/watch?v=cbXRfYBIt0w">video</a>
              <br>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="co2_stop()" onmouseover="co2_start()">
            <td style="padding:40px;padding-left:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='co2_image'><img style="width: 130%; height: auto;" src='images_new/co2_after.png'></div>
                <img style="width: 130%; height: auto;" src='images_new/co2_before.png'>
              </div>
              <script type="text/javascript">
                function co2_start() {
                  document.getElementById('co2_image').style.opacity = "1";
                }

                function co2_stop() {
                  document.getElementById('co2_image').style.opacity = "0";
                }
                co2_stop()
              </script>
            </td>
            <td style="padding:40px;padding-top:65px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2010.02217">
                <papertitle> CO2: Consistent Contrast for Unsupervised Visual Representation Learning</papertitle>
              </a>
              <br>
              <strong>Chen Wei</strong>,
              <a href="https://csrhddlam.github.io/", class="author">Huiyu Wang</a>,
              <a href="https://shenwei1231.github.io/", class="author">Wei Shen</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>
              <br>
              <em>ICLR </em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2010.02217">arXiv</a> /
              <a href="https://openreview.net/forum?id=U4XLJhqwNF1">Open Review</a> /
              <a href="https://iclr.cc/virtual/2021/poster/3354">video</a>
              <br>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="puzzle_stop()" onmouseover="puzzle_start()">
            <td style="padding:40px;padding-left:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='puzzle_image'><img style="width: 130%; height: auto;" src='images_new/puzzle_after.png'></div>
                <img style="width: 130%; height: auto;" src='images_new/puzzle_before.png'>
              </div>
              <script type="text/javascript">
                function puzzle_start() {
                  document.getElementById('puzzle_image').style.opacity = "1";
                }

                function puzzle_stop() {
                  document.getElementById('puzzle_image').style.opacity = "0";
                }
                puzzle_stop()
              </script>
            </td>
            <td style="padding:40px;;padding-top:70px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1812.00329">
                <papertitle> Iterative Reorganization with Weak Spatial Constraints: </br> Solving Arbitrary Jigsaw Puzzles for Unsupervised Representation Learning</papertitle>
              </a>
              <br>
              <strong>Chen Wei</strong>,
              <a href="http://lingxixie.com/", class="author">Lingxi Xie</a>,
              <a href="https://tonghelen.github.io/", class="author">Xutong Ren</a>,
              <a href="http://yingdaxia.github.io/", class="author">Yingda Xia</a>,
              <a href="https://scholar.google.com/citations?user=V-AVamQAAAAJ&hl", class="author">Chi Su</a>,
              <a href="http://www.icst.pku.edu.cn/struct/people/liujiaying.html", class="author">Jiaying Liu</a>,
              <a href="http://www.cs.utsa.edu/~qitian/", class="author">Qi Tian</a>,
              <a href="http://cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1812.00329">arXiv</a> /
              <a href="https://github.com/weichen582/Unsupervised-Visual-Recognition-by-Solving-Arbitrary-Puzzles">code</a>
              <br>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="retinex_stop()" onmouseover="retinex_start()">
            <td style="padding:40px;padding-left:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='retinex_image'><img style="width: 130%; height: auto;" src='images_new/retinex_after.png'></div>
                <img style="width: 130%; height: auto;" src='images_new/retinex_before.png'>
              </div>
              <script type="text/javascript">
                function retinex_start() {
                  document.getElementById('retinex_image').style.opacity = "1";
                }

                function retinex_stop() {
                  document.getElementById('retinex_image').style.opacity = "0";
                }
                retinex_stop()
              </script>
            </td>

            <td style="padding:40px;padding-top:105px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1808.04560">
                <papertitle> Deep Retinex Decomposition for Low-Light Enhancement </papertitle>
              </a>
              <br>
              <strong>Chen Wei*</strong>,
              <a href="https://daooshee.github.io/website/", class="author">Wenjing Wang</a>*,
              <a href="https://flyywh.github.io/index.html", class="author">Wenhan Yang</a>,
              <a href="http://www.icst.pku.edu.cn/struct/people/liujiaying.html", class="author">Jiaying Liu</a>
              <br>
              <em>BMVC</em>, 2018 <strong>Oral</strong>
              <a href="https://arxiv.org/abs/1808.04560">arXiv</a> /
              <a href="https://github.com/weichen582/RetinexNet">code</a> /
              <a href="https://daooshee.github.io/BMVC2018website/">project page & dataset</a>
              <br>
              <br>
              <a href="https://scholar.google.com/citations?hl=en&vq=eng_computervisionpatternrecognition&view_op=list_hcore&venue=T_DfB2ikUbwJ.2022">Most Cited BMVC Papers Over the Last Five Years No.2</a>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="glad_stop()" onmouseover="glad_start()">
            <td style="padding:40px;padding-left:20px;padding-top:20px;width:30%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='glad_image'><img style="width: 130%; height: auto;" src='images_new/glad_after.png'></div>
                <img style="width: 130%; height: auto;" src='images_new/glad_before.png'>
              </div>
              <script type="text/javascript">
                function glad_start() {
                  document.getElementById('glad_image').style.opacity = "1";
                }

                function glad_stop() {
                  document.getElementById('glad_image').style.opacity = "0";
                }
                glad_stop()
              </script>
            </td>
    
            <td style="padding:40px;width:70%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8373911">
                <papertitle> GLADNet: Low-Light Enhancement Network with Global Awareness</papertitle>
              </a>
              <br>
              <a href="https://daooshee.github.io/website/", class="author">Wenjing Wang</a>*,
              <strong>Chen Wei*</strong>,
              <a href="https://flyywh.github.io/index.html", class="author">Wenhan Yang</a>,
              <a href="http://www.icst.pku.edu.cn/struct/people/liujiaying.html", class="author">Jiaying Liu</a>
              <br>
              <em>FG Workshop</em>, 2018
              <br>
              <a href="http://39.96.165.147/Pub%20Files/2018/wwj_fg2018.pdf">PDF</a> /
              <a href="https://github.com/weichen582/GLADNet">code</a> /
              <a href="https://daooshee.github.io/fgworkshop18Gladnet/">project page</a>
              <br>
              <p></p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding-top:20px;padding-bottom:20px;width:100%;vertical-align:middle">
                  <heading>Experience</heading>
                </td>
              </tr>
              <td width="100%" valign="middle">
              <ul>
                <li>Student Researcher, Google DeepMind, Summer 2023</li>
              </ul>
              <ul>
                <li>Research Intern, Facebook AI Research (Meta AI), Summer 2022</li>
              </ul>
              <ul>
                <li>Research Intern, Facebook AI Research, Summer 2021</li>
              </ul>
              <ul>
                <li>Research Intern, Google Cloud AI, Summer 2020</li>
              </ul>
              <ul>
                <li>Research Intern, Johns Hopkins University, Summer 2018</li>
              </ul>
              <ul>
                <li>Engineering Practium Intern, Google, Summer 2017</li>
              </ul>
              <ul>
                <li>Research Intern, <a href="http://www.icst.pku.edu.cn/struct/">STRUCT</a> Lab, Peking University, 2017 - 2019</li>
              </ul>
              </td>
              </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                  Last update: Nov. 2023 &nbsp&nbsp&nbsp&nbsp <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
